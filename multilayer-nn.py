# -*- coding: utf-8 -*-
"""TP Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vnMUO-t5LTpFBjAxT89X05Kq41a6JXPF

# Importacion de los textos
"""

#Importamos todas las librerias necesarias
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

import matplotlib.pyplot as plt

#Importamos el dataset
csv_data = 'data.csv'

#Utilizamos el parametro header = none dado que no hay fila con las etiquetas.
datos = pd.read_csv(csv_data, header=None)

"""# Preprocesamiento de datos


---


## Eliminacion
Decidimos eliminar todas las filas que tengan todos los datos nulos o vacios, dado que son inutiles y solo generarian incongruencia e imprecision. Sin embargo, no hubo ningun caso aplicable.


1.   Elemento de la lista
2.   Elemento de la lista


Luego eliminamos las columnas que tienen demasiados datos de tipo String, dado que al carecer tanto de informacion como que significan esos datos, al convertirlos a un valor numerico podriamos arrastrar un error. 
"""

datos = datos.dropna(how='all')
datos = datos[datos != '?']
datos = datos.dropna(axis = 0)
datos = datos.drop(columns = [3,4,5,6,12], axis = 1)

"""No encontramos ninguna columna que contenga mas de la mitad de sus datos nulos como para realizar una eliminacion completa de una columna.

Acomodamos las etiquetas de las columnas para trabajar mas comodamente
"""

datos.columns = ['0','1','2','3','4','5','6','7','8','9','10']

"""## Codificacion"""

#Codificamos [a,b] => [0,1].
datos['0'] = np.where(datos['0'] == "a", 0 , 1)

#Codificamos [t,f] => [0,1].
datos['4'] = np.where(datos['4'] == "f", 0 , 1)
datos['5'] = np.where(datos['5'] == "f", 0 , 1)
datos['7'] = np.where(datos['7'] == "f", 0, 1)

#Transformamos Dataframe a un arreglo numpy.
X = datos.iloc[:,0:10].values
y = datos.iloc[:,10:11].values

print('Forma de X: ',X.shape)
print('Forma de y: ',y.shape)

"""## Estandarizacion"""

#Estandarizacion de los datos 
sc = StandardScaler()
X = sc.fit_transform(X)

#Usamos Codificacion en caliente para estandarizar el parametro de la clasificacion: Benigno o maligno.
ohe = OneHotEncoder()
y = ohe.fit_transform(y).toarray()

"""# Armado del modelo de RN


---


"""

#Separamos los datos de entrenamiento y prueba.
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25)

print('Dataset de entrenamiento [X]: ' + str(X_train.shape))
print('Dataset de entrenamiento [y]: ' + str(y_train.shape))
print('-------------------------------')
print('Dataset de prueba [X]: ' + str(X_test.shape))
print('Dataset de prueba [y]: ' + str(y_test.shape))

#Construimos el modelo de red neuronal
model = Sequential()

#Agregamos las capas
model.add(Dense(15, input_dim=10, activation='relu'))
model.add(Dense(2, activation='sigmoid'))

#Compilamos el modelo.
#loss = Funcion de calculo de perdida - categorical_crossentropy
#optimizer = Tecnica de calculo de pesos - adam
learning_rate = 0.002
opt = Adam(learning_rate)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy','categorical_accuracy', 'binary_accuracy'])

#Entrenamos el modelo
history_train = model.fit(X_train, y_train, epochs=50)

y_pred = model.predict(X_test)
#Converting predictions to label
pred = list()
for i in range(len(y_pred)):
    pred.append(np.argmax(y_pred[i]))
#Converting one hot encoded test label to label
test = list()
for i in range(len(y_test)):
    test.append(np.argmax(y_test[i]))

#Evaluacion del modelo
a = accuracy_score(pred,test)
print('Accuracy is:', a*100)

#Testeo del modelo
history_test = model.fit(X_test, y_test, epochs=50 )

"""# Grafica de accuracy"""

plt.plot(history_train.history['accuracy'])
plt.plot(history_test.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.autoscale(enable=False, axis='y')
plt.show()

"""# Grafica de funcion de perdida"""

plt.plot(history_train.history['loss'])
plt.plot(history_test.history['loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Test'], loc='upper left') 
plt.autoscale(enable=False, axis='y')
plt.show()

"""# Segunda prueba"""

#Importamos el dataset
csv_data = 'data.csv'

#Utilizamos el parametro header = none dado que no hay fila con las etiquetas.
datos = pd.read_csv(csv_data, header=None)

datos = datos.dropna(how='all')
datos = datos[datos != '?']
datos = datos.dropna(axis = 0)
datos = datos.drop(columns = [3,4,5,6,12], axis = 1)

datos.columns = ['0','1','2','3','4','5','6','7','8','9','10']

#Codificamos [a,b] => [0,1].
datos['0'] = np.where(datos['0'] == "a", 0 , 1)

#Codificamos [t,f] => [0,1].
datos['4'] = np.where(datos['4'] == "f", 0 , 1)
datos['5'] = np.where(datos['5'] == "f", 0 , 1)
datos['7'] = np.where(datos['7'] == "f", 0, 1)
datos['10'] = np.where(datos['10'] == '-', 0, 1)

#Transformamos Dataframe a un arreglo numpy.
X = datos.iloc[:,0:10].values
y = datos.iloc[:,10:11].values

print('Forma de X: ',X.shape)
print('Forma de y: ',y.shape)

#Estandarizacion de los datos 
sc = StandardScaler()
X = sc.fit_transform(X)

#Separamos los datos de entrenamiento y prueba.
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25)

print('Dataset de entrenamiento [X]: ' + str(X_train.shape))
print('Dataset de entrenamiento [y]: ' + str(y_train.shape))
print('-------------------------------')
print('Dataset de prueba [X]: ' + str(X_test.shape))
print('Dataset de prueba [y]: ' + str(y_test.shape))

#Construimos el modelo de red neuronal
model = Sequential()

#Agregamos las capas
model.add(Dense(10, input_dim=10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

#Compilamos el modelo.
#loss = Funcion de calculo de perdida - binary_crossentropy
#optimizer = Tecnica de calculo de pesos - adam
learning_rate = 0.008
opt = Adam(learning_rate)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy','binary_accuracy'])

#Entrenamos el modelo
history_train = model.fit(X_train, y_train, epochs=20)

y_pred = model.predict(X_test)
#Converting predictions to label
pred = list()
for i in range(len(y_pred)):
    pred.append(np.argmax(y_pred[i]))
#Converting one hot encoded test label to label
test = list()
for i in range(len(y_test)):
    test.append(np.argmax(y_test[i]))

#Evaluacion del modelo
a = accuracy_score(pred,test)
print('Accuracy is:', a*100)

#Testeo del modelo
history_test =  model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=20 )

"""GRAFICA ACURACCY

"""

plt.plot(history_train.history['accuracy'])
plt.plot(history_test.history['accuracy'])
plt.plot(history_test.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test', 'Test Val'], loc='lower left')
plt.autoscale(enable=False, axis='y')
plt.show()

"""GRAFICA PERDIDA"""

plt.plot(history_train.history['loss'])
plt.plot(history_test.history['loss']) 
plt.plot(history_test.history['val_loss']) 
plt.title('Model loss') 
plt.ylabel('Loss') 
plt.xlabel('Epoch') 
plt.legend(['Train', 'Test', 'Test Val'], loc='upper right') 
plt.autoscale(enable=False, axis='y')
plt.show()

"""# Tercera prueba"""

def predict(row, weights):
	activation = weights[0]
	for i in range(len(row)-1):
		activation += weights[i + 1] * row[i]
	return 1.0 if activation >= 0.0 else 0.0
 
# Estimate Perceptron weights using stochastic gradient descent
# Se calculan los pesos para las entradas (dendritas) del perceptron.
def train_weights(train, l_rate, n_epoch):
	weights = [0.0 for i in range(len(train[0]))]
	for epoch in range(n_epoch):
		sum_error = 0.0
		for row in train:
			prediction = predict(row, weights)
			error = row[-1] - prediction
			sum_error += error**2
			weights[0] = weights[0] + l_rate * error * 1 #bias
			for i in range(len(row)-1):
				weights[i + 1] = weights[i + 1] + l_rate * error * row[i]
		print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))
	return weights
 
# Calculo de pesos
dataset = X_train
l_rate = 0.01
n_epoch = 100
weights = train_weights(dataset, l_rate, n_epoch)
print(weights)

# Calculo de pesos
dataset = X_test
l_rate = 0.01
n_epoch = 100

weights = train_weights(dataset, l_rate, n_epoch)
print(weights)
weights = np.array(weights)
weights = weights.reshape(10, 1)

"""# Cuarta prueba"""

def predict(row, weights):
	activation = weights[0]
	for i in range(len(row)-1):
		activation += weights[i + 1] * row[i]
	return 1.0 if activation >= 0.0 else 0.0
 
# Estimate Perceptron weights using stochastic gradient descent
# Se calculan los pesos para las entradas (dendritas) del perceptron.
def train_weights(train, l_rate, n_epoch):
	weights = [0.0 for i in range(len(train[0]))]
	for epoch in range(n_epoch):
		sum_error = 0.0
		for row in train:
			prediction = predict(row, weights)
			error = row[-1] - prediction
			sum_error += error**2
			weights[0] = weights[0] + l_rate * error * 1 #bias
			for i in range(len(row)-1):
				weights[i + 1] = weights[i + 1] + l_rate * error * row[i]
		print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))
	return weights
 
# Calculo de pesos
dataset = X_train
l_rate = 0.02
n_epoch = 50
weights = train_weights(dataset, l_rate, n_epoch)
print(weights)

# Calculo de pesos
dataset = X_test
l_rate = 0.02
n_epoch = 50

weights = train_weights(dataset, l_rate, n_epoch)
print(weights)
weights = np.array(weights)
weights = weights.reshape(10, 1)